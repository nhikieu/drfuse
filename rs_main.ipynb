{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from models import DrFuse_RS_Model, DrFuse_RS_Trainer\n",
    "import time\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "import skimage.io as io\n",
    "import earthpy.spatial as es\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints\\1720751784\n"
     ]
    }
   ],
   "source": [
    "now = str(int(time.time()))\n",
    "\n",
    "EXPERIMENT_CONFIG = dict(\n",
    "    # window data folder\n",
    "    DATA_FOLDER = r'D:\\CarsSegmentationTroubleShoot\\data\\multimodal',\n",
    "    # mac data folder\n",
    "    # DATA_FOLDER = r'/Users/nhikieu/Documents/PhD/multimodal',\n",
    "    num_epoch = 100, \n",
    "    img_size = 512, \n",
    "    num_classes = 6,\n",
    "    input_channels = 5,\n",
    "    batch_size = 4,\n",
    "    checkpoint_pth = os.path.join('checkpoints', now),\n",
    "    update_optim_size = 32\n",
    "    )\n",
    "DEVICE = 'mps'\n",
    "\n",
    "print(EXPERIMENT_CONFIG['checkpoint_pth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_class(mask_image):\n",
    "  class_map = {\n",
    "    (255,255,255): 0, \n",
    "    (0,0,255): 1, \n",
    "    (0,255,255): 2, \n",
    "    (0,255,0): 3, \n",
    "    (255,255,0): 4, \n",
    "    (255,0,0): 5\n",
    "  }\n",
    "\n",
    "  # Create a 3D numpy array that represents the RGB color of each pixel\n",
    "  rgb_data = mask_image.reshape(-1, 3)\n",
    "\n",
    "  # Create a 1D numpy array that represents the class label for each RGB color\n",
    "  class_labels = np.zeros(rgb_data.shape[0], dtype=np.uint8)\n",
    "  for rgb, class_label in class_map.items():\n",
    "      mask = np.all(rgb_data == np.array(rgb), axis=1)\n",
    "      class_labels[mask] = class_label\n",
    "\n",
    "  # Reshape the 1D class label array into a 2D class map\n",
    "  class_data = class_labels.reshape(mask_image.shape[:2])\n",
    "\n",
    "  return class_data\n",
    "\n",
    "\n",
    "class VaihingenDataset(Dataset):\n",
    "  def __init__(self, folder) -> None:\n",
    "    '''\n",
    "    folder: data path include both rgb img and gt\n",
    "    gt: 3 channels map with postfix '_gt'\n",
    "    '''\n",
    "    super().__init__()\n",
    "\n",
    "    self.imgs = []\n",
    "    self.gts = []\n",
    "\n",
    "    # get all filenames in the directory\n",
    "    filenames = os.listdir(os.path.join(folder, 'rgb'))\n",
    "\n",
    "    for f in tqdm(filenames):\n",
    "      img = io.imread(os.path.join(folder, 'rgb', f)) / 255.0\n",
    "     \n",
    "      ndsm = io.imread(os.path.join(folder, 'ndsm', f.split('.png')[0] + '_ndsm.tif'))\n",
    "      ndsm = np.expand_dims(ndsm, axis=2)\n",
    "    \n",
    "      input_4C = np.dstack((img, ndsm))\n",
    "\n",
    "      input_4C = torch.tensor(input_4C).permute((2, 0, 1))\n",
    "      \n",
    "      gt_path = os.path.join(folder, 'gt', f.split('.png')[0] + '_gt.png')\n",
    "      gt = io.imread(gt_path)\n",
    "      gt = rgb_to_class(gt)\n",
    "      gt = torch.tensor(gt).unsqueeze(0)\n",
    "\n",
    "      self.imgs.append(input_4C)\n",
    "      self.gts.append(gt)\n",
    "    \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.imgs)\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    input_4C = self.imgs[index].float()\n",
    "    gt = self.gts[index]\n",
    "\n",
    "    # mask_array = [True, False]\n",
    "    mask_index = np.random.choice(2, 1, p=[0.5, 0.5])\n",
    "    mask = torch.tensor(mask_index[0])\n",
    "\n",
    "    return input_4C, gt, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1620/1620 [03:08<00:00,  8.61it/s]\n",
      "100%|██████████| 120/120 [00:11<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1620\n",
      "Val samples: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Note that we are using ground truth in the folder '1CGT'. They are not RGB anymore, but one channel.\n",
    "training_dataset = VaihingenDataset(os.path.join(EXPERIMENT_CONFIG['DATA_FOLDER'], 'train'))\n",
    "validate_dataset = VaihingenDataset(os.path.join(EXPERIMENT_CONFIG['DATA_FOLDER'], 'val'))\n",
    "\n",
    "#Note that we shuffle the data for the training set, but not for the validation set:\n",
    "train_loader = DataLoader(dataset=training_dataset, batch_size=EXPERIMENT_CONFIG['batch_size'], shuffle=True, pin_memory=True)\n",
    "validate_loader = DataLoader(dataset=validate_dataset, batch_size=EXPERIMENT_CONFIG['batch_size'], shuffle=True, pin_memory=True)\n",
    "\n",
    "print(f'Train samples: {len(training_dataset)}')\n",
    "print(f'Val samples: {len(validate_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnhikieu\u001b[0m (\u001b[33mqut_nhi_phd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnhikieu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\drfuse\\wandb\\run-20240712_123950-9ipn34ng</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nhikieu/drfuse/runs/9ipn34ng' target=\"_blank\">dandy-eon-23</a></strong> to <a href='https://wandb.ai/nhikieu/drfuse' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nhikieu/drfuse' target=\"_blank\">https://wandb.ai/nhikieu/drfuse</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nhikieu/drfuse/runs/9ipn34ng' target=\"_blank\">https://wandb.ai/nhikieu/drfuse/runs/9ipn34ng</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]d:\\drfuse\\models\\drfuse_rs.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = (images.to(device), labels.to(device), torch.tensor(masks).to(device))\n",
      "d:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:80: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
      "  0%|          | 0/150 [03:27<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\n10300759\\AppData\\Local\\Temp\\ipykernel_28720\\866226422.py\", line 6, in <module>\n",
      "    model_trainer(train_loader, validate_loader)\n",
      "  File \"d:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"d:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"d:\\drfuse\\models\\drfuse_rs.py\", line 93, in forward\n",
      "    train_loss = self.training_step(batch, log, self.pretrain)\n",
      "  File \"d:\\drfuse\\models\\drfuse_rs.py\", line 150, in training_step\n",
      "    return train_loss.item()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dandy-eon-23</strong> at: <a href='https://wandb.ai/nhikieu/drfuse/runs/9ipn34ng' target=\"_blank\">https://wandb.ai/nhikieu/drfuse/runs/9ipn34ng</a><br/> View project at: <a href='https://wandb.ai/nhikieu/drfuse' target=\"_blank\">https://wandb.ai/nhikieu/drfuse</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240712_123950-9ipn34ng\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m      5\u001b[0m model_trainer \u001b[38;5;241m=\u001b[39m DrFuse_RS_Trainer(config, pretrain\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ckpt\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\drfuse\\models\\drfuse_rs.py:93\u001b[0m, in \u001b[0;36mDrFuse_RS_Trainer.forward\u001b[1;34m(self, train_loader, val_loader, device)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader):\n",
      "File \u001b[1;32md:\\drfuse\\models\\drfuse_rs.py:150\u001b[0m, in \u001b[0;36mDrFuse_RS_Trainer.training_step\u001b[1;34m(self, batch, log, pretrain)\u001b[0m\n\u001b[0;32m    146\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_and_log_loss(out, y_gt\u001b[38;5;241m=\u001b[39my, log\u001b[38;5;241m=\u001b[39mlog, pretrain\u001b[38;5;241m=\u001b[39mpretrain)\n\u001b[0;32m    148\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# best pretrained path\n",
    "ckpt_path = r\"D:\\drfuse\\checkpoints\\1720700298\\1720724395_loss1.8435\"\n",
    "with wandb.init(project=\"drfuse\", entity=\"nhikieu\", config=EXPERIMENT_CONFIG):\n",
    "    config = wandb.config\n",
    "    model_trainer = DrFuse_RS_Trainer(config, pretrain=True, ckpt=ckpt_path)\n",
    "    model_trainer(train_loader, validate_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\drfuse\\drfuse_venv\\lib\\site-packages\\torch\\nn\\modules\\instancenorm.py:80: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
      "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6, 512, 512]) torch.Size([6, 6, 512, 512]) torch.Size([6, 6, 512, 512]) torch.Size([6, 6, 512, 512]) torch.Size([6, 6, 512, 512])\n",
      "Total trainable parameters: 42120024\n"
     ]
    }
   ],
   "source": [
    "dummies = torch.rand(6, 4, 512, 512)\n",
    "masks = torch.FloatTensor([True, False, True, True, False, False])\n",
    "model = DrFuse_RS_Model()\n",
    "results = model(dummies, masks)\n",
    "print(results['pred_rgb'].shape, results['pred_ndsm'].shape, \n",
    "      results['pred_shared'].shape, results['pred_multimodal'].shape,\n",
    "      results['aux_preds'][0].shape)\n",
    "\n",
    "# Assuming model is an instance of a class derived from torch.nn.Module\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = torch.FloatTensor([True, False, True, False, True, True])\n",
    "pairs = pairs.unsqueeze(1)\n",
    "feat_dummy = torch.randn(6, 512)\n",
    "test = pairs*feat_dummy\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_4C, gt, mask) in enumerate(validate_loader):\n",
    "  print(input_4C.shape, gt.shape, mask.shape)\n",
    "  print(mask)\n",
    "  break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drfuse_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
